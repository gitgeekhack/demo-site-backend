import os
import boto3
from fuzzywuzzy import fuzz

from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.llms.bedrock import Bedrock
from langchain.prompts import PromptTemplate
from langchain.docstore.document import Document
from langchain.embeddings import BedrockEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from app.constant import MedicalInsights
from app.service.medical_document_insights.nlp_extractor import bedrock_client


class DocumentSummarizer:
    def __init__(self):
        os.environ['AWS_DEFAULT_REGION'] = MedicalInsights.AWS_DEFAULT_REGION
        self.bedrock_client = bedrock_client
        self.model_id_llm = 'anthropic.claude-v2'
        self.model_embeddings = 'amazon.titan-embed-text-v1'

        self.llm = Bedrock(
            model_id=self.model_id_llm,
            model_kwargs={
                "max_tokens_to_sample": 10000,
                "temperature": 0.5,
                "top_p": 0.9,
                "top_k": 250,
            },
            client=self.bedrock_client,
        )

        self.bedrock_embeddings = BedrockEmbeddings(model_id=self.model_embeddings, client=self.bedrock_client)

    async def __data_formatter(self, json_data):
        """ This method is used to format the data and prepare chunks """

        raw_text = "".join(json_data.values())
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=200)
        texts = text_splitter.split_text(raw_text)

        docs = [Document(page_content=t) for t in texts]
        return docs

    async def __remove_first_line(self, generated_summary_first_line):
        reference_summary_first_line = 'Based on the provided medical report, here is a summary of the key information:'
        matching_threshold = 60
        return fuzz.token_sort_ratio(reference_summary_first_line, generated_summary_first_line) > matching_threshold

    async def __post_processing(self, summary):
        """ This method is used to post-process the summary generated by LLM"""

        text = summary.replace('- ', '')
        text = text.strip()
        lines = text.split('\n')
        if await self.__remove_first_line(lines[0]):
            lines = lines[1:]

        if lines[-1].__contains__('?'):
            lines = lines[:-1]

        modified_text = '\n'.join(lines)
        return modified_text.strip()

    async def get_summary(self, json_data):
        """ This method is used to get the summary of document """

        docs = await self.__data_formatter(json_data)

        vectorstore_faiss = FAISS.from_documents(
            documents=docs,
            embedding=self.bedrock_embeddings,
        )

        query = """Generate a detailed and accurate summary based on the user's input. Specifically, concentrate on identifying key medical diagnoses, outlining treatment plans, and highlighting pertinent aspects of the medical history. Strive for precision and conciseness to deliver a focused and insightful summary."""

        prompt_template = """
        Human: Use the following pieces of context to provide a concise answer to the question at the end. If you don't know the answer, don't try to make up an answer.
        <context>
        {context}
        </context>

        Question: {question}

        Assistant:"""

        prompt = PromptTemplate(
            template=prompt_template, input_variables=["context", "question"]
        )

        qa = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=vectorstore_faiss.as_retriever(
                search_type="similarity", search_kwargs={"k": 6}
            ),
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt}
        )

        answer = qa({"query": query})
        response = answer['result']

        summary = await self.__post_processing(response)
        return {"summary": summary}
